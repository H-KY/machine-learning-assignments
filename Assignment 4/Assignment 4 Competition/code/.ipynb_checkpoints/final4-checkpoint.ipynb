{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/dual/cs5150278/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping # early stopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash = {}  #mappings of string label with int label\n",
    "num_classes = 20 #total number of classes\n",
    "test_size = 0.20 #training and validation split\n",
    "img_rows = 28 #number of rows\n",
    "img_cols = 28 #number of columns\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "batch_size = 64 # in each iteration, we consider 64 training examples at once\n",
    "num_epochs = 20 # we iterate twelve times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # there will be 128 neurons in both hidden layers\n",
    "\n",
    "num_train = 100000 # there are 60000 training examples in train set\n",
    "num_test = 100000 # there are 10000 test examples in test set\n",
    "\n",
    "l2_lambda = 0.0001 #lambda for the l2 regularization\n",
    "ens_models = 3 # we will train three separate models on the data\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    return np.array(X, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "train_y = []\n",
    "train_X = []\n",
    "test_X = []\n",
    "flag = 0\n",
    "label = 0\n",
    "for filename in os.listdir(train_directory):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        path_to_file = os.path.join(train_directory, filename)\n",
    "        X = np.load(path_to_file)\n",
    "        y = os.path.splitext(filename)[0]\n",
    "        if(flag == 0):\n",
    "            train_X = X\n",
    "            flag = 1\n",
    "        else:\n",
    "            train_X = np.vstack((train_X, X))\n",
    "        for i in range(X.shape[0]):\n",
    "            train_y.append(label)\n",
    "        hash[label] = y\n",
    "        label += 1\n",
    "test_X = np.load(os.path.join(test_directory, 'test.npy'))\n",
    "\n",
    "y = keras.utils.to_categorical(np.array(train_y), num_classes)\n",
    "train_X = preprocess(train_X)\n",
    "test_X = preprocess(test_X)\n",
    "X = (train_X.reshape(train_X.shape[0], img_rows, img_cols, 1)).astype('float32')\n",
    "test_X = (test_X.reshape(test_X.shape[0], img_rows, img_cols, 1)).astype('float32')\n",
    "\n",
    "X = (X * 1.0) / 255.0\n",
    "test_X = (test_X * 1.0) / 255.0\n",
    "\n",
    "tr_X, te_X, tr_y, te_y = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(conv_depth, (kernel_size, kernel_size), input_shape = input_shape, activation = 'relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(Conv2D(conv_depth, (kernel_size, kernel_size), activation = 'relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Conv2D(conv_depth, (kernel_size, kernel_size), activation = 'relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(Conv2D(conv_depth, (kernel_size, kernel_size), activation = 'relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "# model.add(Dropout(drop_prob_1))\n",
    "\n",
    "#flatten the model\n",
    "model.add(Flatten())\n",
    "\n",
    "#feed into a fully connected hidden layer with 512 units\n",
    "model.add(Dense(units=512, activation='relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(Dense(units=128, activation='relu', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#get the multilcass classification using softmax regression \n",
    "model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=l2(l2_lambda), kernel_initializer='glorot_normal'))\n",
    "    \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range = 8, shear_range = 0.3, height_shift_range = 0.08, zoom_range = 0.08)\n",
    "gen.fit(tr_X)\n",
    "test_gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(tr_X, tr_y, batch_size = batch_size)\n",
    "test_generator = test_gen.flow(te_X, te_y, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1093/1093 [==============================] - 26s 24ms/step - loss: 0.4558 - acc: 0.9090 - val_loss: 0.6979 - val_acc: 0.8499\n",
      "Epoch 2/10\n",
      "1093/1093 [==============================] - 26s 24ms/step - loss: 0.4735 - acc: 0.9040 - val_loss: 0.5783 - val_acc: 0.8805\n",
      "Epoch 3/10\n",
      "1093/1093 [==============================] - 26s 24ms/step - loss: 0.4698 - acc: 0.9055 - val_loss: 0.5141 - val_acc: 0.8963\n",
      "Epoch 4/10\n",
      "1093/1093 [==============================] - 26s 24ms/step - loss: 0.4642 - acc: 0.9063 - val_loss: 0.5173 - val_acc: 0.8933\n",
      "Epoch 5/10\n",
      "1090/1093 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.9065"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch = tr_X.shape[0]//batch_size, epochs =15, validation_data = test_generator, validation_steps = te_X.shape[0]//batch_size, verbose=1, shuffle = True, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_best_model_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('my_best_model_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model , vary the number of epochs, and the batch size\n",
    "y_pred = model.predict(test_X)\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('4.csv', 'w')\n",
    "f.write(\"ID,CATEGORY\\n\")\n",
    "for i in range(len(y_pred)):\n",
    "    f.write(str(i) + \",\" + hash[y_pred[i]] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
